{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_path = '/home/umera_p/Dataset/training/memes'\n",
    "\n",
    "def display_images_grid(image_paths, title):\n",
    "    fig, axes = plt.subplots(nrows=10, ncols=5, figsize=(15, 30))\n",
    "    fig.suptitle(title, fontsize=20)\n",
    "    for ax, image_path in zip(axes.flatten(), image_paths):\n",
    "        try:\n",
    "            image = Image.open(image_path)\n",
    "            ax.imshow(image)\n",
    "            ax.axis('off')\n",
    "        except FileNotFoundError:\n",
    "            ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(top=0.95)\n",
    "    plt.show()\n",
    "\n",
    "spanish_ids = [str(i) for i in range(110001, 112035)]\n",
    "english_ids = [str(i) for i in range(210001, 212011)]\n",
    "\n",
    "spanish_images = []\n",
    "for image_id in spanish_ids[:50]:\n",
    "    for ext in ['jpeg', 'jpg', 'png']:\n",
    "        image_path = os.path.join(images_path, f\"{image_id}.{ext}\")\n",
    "        if os.path.exists(image_path):\n",
    "            spanish_images.append(image_path)\n",
    "            break\n",
    "\n",
    "english_images = []\n",
    "for image_id in english_ids[:50]:\n",
    "    for ext in ['jpeg', 'jpg', 'png']:\n",
    "        image_path = os.path.join(images_path, f\"{image_id}.{ext}\")\n",
    "        if os.path.exists(image_path):\n",
    "            english_images.append(image_path)\n",
    "            break\n",
    "\n",
    "display_images_grid(spanish_images, 'First 50 Spanish Images')\n",
    "\n",
    "display_images_grid(english_images, 'First 50 English Images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations_path = '/home/umera_p/Dataset/training/training.json'\n",
    "with open(annotations_path, 'r') as file:\n",
    "    annotations = json.load(file)\n",
    "\n",
    "df = pd.DataFrame.from_dict(annotations, orient='index')\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sexist_count = sum(1 for value in annotations.values() if value['labels_task4'].count('YES')>value['labels_task4'].count('NO'))\n",
    "non_sexist_count = len(annotations) - sexist_count\n",
    "sexist_count_es = sum(1 for value in annotations.values() if value['lang'] == 'es' and value['labels_task4'].count('YES')>value['labels_task4'].count('NO'))\n",
    "non_sexist_count_es = sum(1 for value in annotations.values() if value['lang'] == 'es' and value['labels_task4'].count('YES')<=value['labels_task4'].count('NO'))\n",
    "sexist_count_en = sum(1 for value in annotations.values() if value['lang'] == 'en' and value['labels_task4'].count('YES')>value['labels_task4'].count('NO'))\n",
    "non_sexist_count_en = sum(1 for value in annotations.values() if value['lang'] == 'en' and value['labels_task4'].count('YES')<=value['labels_task4'].count('NO'))\n",
    "\n",
    "print(f\"Total number of images: {len(annotations)}\")\n",
    "print(f\"Number of sexist images: {sexist_count}\")\n",
    "print(f\"Number of sexist images (Spanish): {sexist_count_es}\")\n",
    "print(f\"Number of sexist images (English): {sexist_count_en}\")\n",
    "print(f\"Number of non-sexist images: {non_sexist_count}\")\n",
    "print(f\"Number of non-sexist images (Spanish): {non_sexist_count_es}\")\n",
    "print(f\"Number of non-sexist images (English): {non_sexist_count_en}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torchvision import transforms "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_transform = transforms.Compose([\n",
    "    transforms.Resize(256),  \n",
    "    transforms.CenterCrop(224),  \n",
    "    transforms.ToTensor(),  \n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  \n",
    "])\n",
    "\n",
    "extensions = ['.jpeg', '.jpg', '.png']\n",
    "preprocessed_images = []\n",
    "labels_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for image_id, label in image_labels.items():\n",
    "    image_loaded = False\n",
    "    for ext in extensions:\n",
    "        filename = f\"{image_id}{ext}\"\n",
    "        image_path = os.path.join(dataset_path, filename)\n",
    "        if os.path.exists(image_path):\n",
    "            try:\n",
    "                image = Image.open(image_path).convert(\"RGB\")  \n",
    "                preprocessed_image = preprocess_transform(image)\n",
    "                preprocessed_images.append(preprocessed_image)\n",
    "                labels_list.append(label)  \n",
    "                image_loaded = True\n",
    "                break  \n",
    "            except Exception as e:\n",
    "                print(f\"Failed to process image {filename}: {e}\")\n",
    "    \n",
    "    if not image_loaded:\n",
    "        print(f\"Image {image_id} could not be loaded with any known extension.\")\n",
    "\n",
    "preprocessed_images_tensor = torch.stack(preprocessed_images) if preprocessed_images else None\n",
    "encoded_labels_tensor = torch.tensor(labels_list, dtype=torch.long) if labels_list else None\n",
    "\n",
    "if preprocessed_images_tensor is not None and encoded_labels_tensor is not None:\n",
    "    dataset = TensorDataset(preprocessed_images_tensor, encoded_labels_tensor)\n",
    "\n",
    "    dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "    print(\"Shape of preprocessed images tensor:\", preprocessed_images_tensor.shape)\n",
    "    print(\"Shape of encoded labels tensor:\", encoded_labels_tensor.shape)\n",
    "else:\n",
    "    print(\"No images were loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip list\n",
    "# import sys\n",
    "# print(sys.executable)\n",
    "# !pip install clip\n",
    "# pip install gitpython\n",
    "# pip install torch torchvision torchaudio\n",
    "# ! pip install ftfy regex tqdm\n",
    "# pip install --upgrade clip\n",
    "# ls \"C:\\Users\\Umera Pasha\\Desktop\\MASTERS THESIS PREP\\CLIP\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import clip\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip.available_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_image_features(image_paths, batch_size=32):\n",
    "    image_features = []\n",
    "    for i in range(0, len(image_paths), batch_size):\n",
    "        batch_paths = image_paths[i:i+batch_size]\n",
    "        batch_images = torch.stack([preprocess(Image.open(path).convert('RGB')).to(device) for path in batch_paths])\n",
    "        with torch.no_grad():\n",
    "            batch_features = model.encode_image(batch_images)\n",
    "        image_features.append(batch_features.cpu().numpy())\n",
    "    return np.concatenate(image_features, axis=0)\n",
    "\n",
    "def get_all_image_paths(dataset_path, extensions=['.jpeg', '.jpg', '.png']):\n",
    "    image_paths = []\n",
    "    for root, dirs, files in os.walk(dataset_path):\n",
    "        for file in files:\n",
    "            if any(file.endswith(ext) for ext in extensions):\n",
    "                image_paths.append(os.path.join(root, file))\n",
    "    return image_paths\n",
    "\n",
    "image_paths = get_all_image_paths(dataset_path)\n",
    "\n",
    "image_features = extract_image_features(image_paths)\n",
    "\n",
    "print(\"Shape of extracted features array:\", image_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stratified_split(dataset, split_ratios=(0.8, 0.1, 0.1)):\n",
    "    assert sum(split_ratios) == 1.0, \"Split ratios must sum to 1.0\"\n",
    "\n",
    "    non_sexist_es = [key for key, value in dataset.items() if value['lang'] == 'es' and value['labels_task4'].count('YES')<=value['labels_task4'].count('NO')]\n",
    "    non_sexist_en = [key for key, value in dataset.items() if value['lang'] == 'en' and value['labels_task4'].count('YES')<=value['labels_task4'].count('NO')]\n",
    "    sexist_es = [key for key, value in dataset.items() if value['lang'] == 'es' and value['labels_task4'].count('YES')>value['labels_task4'].count('NO')]\n",
    "    sexist_en = [key for key, value in dataset.items() if value['lang'] == 'en' and value['labels_task4'].count('YES')>value['labels_task4'].count('NO')]\n",
    "\n",
    "    random.shuffle(non_sexist_es)\n",
    "    random.shuffle(non_sexist_en)\n",
    "    random.shuffle(sexist_es)\n",
    "    random.shuffle(sexist_en)\n",
    "\n",
    "    def split_list(data_list, split_ratios):\n",
    "        train_size = round(int(split_ratios[0] * len(data_list)))\n",
    "        val_size = round(int(split_ratios[1] * len(data_list)))\n",
    "        train_split = data_list[:train_size]\n",
    "        val_split = data_list[train_size:train_size + val_size]\n",
    "        test_split = data_list[train_size + val_size:]\n",
    "        return train_split, val_split, test_split\n",
    "\n",
    "    train_non_sexist_es, val_non_sexist_es, test_non_sexist_es = split_list(non_sexist_es, split_ratios)\n",
    "    train_non_sexist_en, val_non_sexist_en, test_non_sexist_en = split_list(non_sexist_en, split_ratios)\n",
    "    train_sexist_es, val_sexist_es, test_sexist_es = split_list(sexist_es, split_ratios)\n",
    "    train_sexist_en, val_sexist_en, test_sexist_en = split_list(sexist_en, split_ratios)\n",
    "\n",
    "    train_keys = train_non_sexist_es + train_non_sexist_en + train_sexist_es + train_sexist_en\n",
    "    val_keys = val_non_sexist_es + val_non_sexist_en + val_sexist_es + val_sexist_en\n",
    "    test_keys = test_non_sexist_es + test_non_sexist_en + test_sexist_es + test_sexist_en\n",
    "\n",
    "    random.shuffle(train_keys)\n",
    "    random.shuffle(val_keys)\n",
    "    random.shuffle(test_keys)\n",
    "\n",
    "    return train_keys, val_keys, test_keys\n",
    "\n",
    "train_keys, val_keys, test_keys = stratified_split(annotations)\n",
    "\n",
    "print(f\"Training set size: {len(train_keys)}\")\n",
    "print(f\"Validation set size: {len(val_keys)}\")\n",
    "print(f\"Known test set size: {len(test_keys)}\")\n",
    "\n",
    "print(\"\\nTraining IDs:\", train_keys)\n",
    "print(\"\\nValidation IDs:\", val_keys)\n",
    "print(\"\\nKnown Test IDs:\", test_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import random\n",
    "\n",
    "def get_features_and_labels(keys, feature_array, annotations, id_to_index):\n",
    "    features = []\n",
    "    labels = []\n",
    "    for key in keys:\n",
    "        index = id_to_index[key]\n",
    "        features.append(feature_array[index])\n",
    "        labels.append(1 if annotations[key]['labels_task4'].count('YES') > annotations[key]['labels_task4'].count('NO') else 0)\n",
    "    return np.array(features), np.array(labels)\n",
    "\n",
    "id_to_index = {key: i for i, key in enumerate(annotations.keys())}\n",
    "\n",
    "train_features, train_labels = get_features_and_labels(train_keys, image_features, annotations, id_to_index)\n",
    "val_features, val_labels = get_features_and_labels(val_keys, image_features, annotations, id_to_index)\n",
    "test_features, test_labels = get_features_and_labels(test_keys, image_features, annotations, id_to_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_model = LogisticRegression(max_iter=1000) \n",
    "\n",
    "logistic_model.fit(train_features, train_labels)\n",
    "\n",
    "val_predictions = logistic_model.predict(val_features)\n",
    "test_predictions = logistic_model.predict(test_features)\n",
    "\n",
    "val_accuracy = accuracy_score(val_labels, val_predictions)\n",
    "print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
    "print(\"Validation Classification Report:\")\n",
    "print(classification_report(val_labels, val_predictions))\n",
    "\n",
    "test_accuracy = accuracy_score(test_labels, test_predictions)\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "print(\"Test Classification Report:\")\n",
    "print(classification_report(test_labels, test_predictions))\n",
    "\n",
    "print(\"Validation Confusion Matrix:\")\n",
    "print(confusion_matrix(val_labels, val_predictions))\n",
    "\n",
    "print(\"Test Confusion Matrix:\")\n",
    "print(confusion_matrix(test_labels, test_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "random_forest_model = RandomForestClassifier(n_estimators=100, random_state=42) \n",
    "\n",
    "random_forest_model.fit(train_features, train_labels)\n",
    "\n",
    "val_predictions = random_forest_model.predict(val_features)\n",
    "test_predictions = random_forest_model.predict(test_features)\n",
    "\n",
    "val_accuracy = accuracy_score(val_labels, val_predictions)\n",
    "print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
    "print(\"Validation Classification Report:\")\n",
    "print(classification_report(val_labels, val_predictions))\n",
    "\n",
    "test_accuracy = accuracy_score(test_labels, test_predictions)\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "print(\"Test Classification Report:\")\n",
    "print(classification_report(test_labels, test_predictions))\n",
    "\n",
    "print(\"Validation Confusion Matrix:\")\n",
    "print(confusion_matrix(val_labels, val_predictions))\n",
    "\n",
    "print(\"Test Confusion Matrix:\")\n",
    "print(confusion_matrix(test_labels, test_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svm_model = SVC(kernel='linear', C=1.0, random_state=42) \n",
    "\n",
    "svm_model.fit(train_features, train_labels)\n",
    "\n",
    "val_predictions = svm_model.predict(val_features)\n",
    "test_predictions = svm_model.predict(test_features)\n",
    "\n",
    "val_accuracy = accuracy_score(val_labels, val_predictions)\n",
    "print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
    "print(\"Validation Classification Report:\")\n",
    "print(classification_report(val_labels, val_predictions))\n",
    "\n",
    "test_accuracy = accuracy_score(test_labels, test_predictions)\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "print(\"Test Classification Report:\")\n",
    "print(classification_report(test_labels, test_predictions))\n",
    "\n",
    "print(\"Validation Confusion Matrix:\")\n",
    "print(confusion_matrix(val_labels, val_predictions))\n",
    "\n",
    "print(\"Test Confusion Matrix:\")\n",
    "print(confusion_matrix(test_labels, test_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "decision_tree_model = DecisionTreeClassifier(random_state=42) \n",
    "\n",
    "decision_tree_model.fit(train_features, train_labels)\n",
    "\n",
    "val_predictions = decision_tree_model.predict(val_features)\n",
    "test_predictions = decision_tree_model.predict(test_features)\n",
    "\n",
    "val_accuracy = accuracy_score(val_labels, val_predictions)\n",
    "print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
    "print(\"Validation Classification Report:\")\n",
    "print(classification_report(val_labels, val_predictions))\n",
    "\n",
    "test_accuracy = accuracy_score(test_labels, test_predictions)\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "print(\"Test Classification Report:\")\n",
    "print(classification_report(test_labels, test_predictions))\n",
    "\n",
    "print(\"Validation Confusion Matrix:\")\n",
    "print(confusion_matrix(val_labels, val_predictions))\n",
    "\n",
    "print(\"Test Confusion Matrix:\")\n",
    "print(confusion_matrix(test_labels, test_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "xgb_model = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
    "\n",
    "xgb_model.fit(train_features, train_labels)\n",
    "\n",
    "val_predictions = xgb_model.predict(val_features)\n",
    "test_predictions = xgb_model.predict(test_features)\n",
    "\n",
    "val_accuracy = accuracy_score(val_labels, val_predictions)\n",
    "print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
    "print(\"Validation Classification Report:\")\n",
    "print(classification_report(val_labels, val_predictions))\n",
    "\n",
    "test_accuracy = accuracy_score(test_labels, test_predictions)\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "print(\"Test Classification Report:\")\n",
    "print(classification_report(test_labels, test_predictions))\n",
    "\n",
    "print(\"Validation Confusion Matrix:\")\n",
    "print(confusion_matrix(val_labels, val_predictions))\n",
    "\n",
    "print(\"Test Confusion Matrix:\")\n",
    "print(confusion_matrix(test_labels, test_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AdaBoost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "adaboost_model = AdaBoostClassifier(n_estimators=50, random_state=42) \n",
    "\n",
    "adaboost_model.fit(train_features, train_labels)\n",
    "\n",
    "val_predictions = adaboost_model.predict(val_features)\n",
    "test_predictions = adaboost_model.predict(test_features)\n",
    "\n",
    "val_accuracy = accuracy_score(val_labels, val_predictions)\n",
    "print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
    "print(\"Validation Classification Report:\")\n",
    "print(classification_report(val_labels, val_predictions))\n",
    "\n",
    "test_accuracy = accuracy_score(test_labels, test_predictions)\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "print(\"Test Classification Report:\")\n",
    "print(classification_report(test_labels, test_predictions))\n",
    "\n",
    "print(\"Validation Confusion Matrix:\")\n",
    "print(confusion_matrix(val_labels, val_predictions))\n",
    "\n",
    "print(\"Test Confusion Matrix:\")\n",
    "print(confusion_matrix(test_labels, test_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SGD Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "sgd_model = SGDClassifier(loss='hinge', max_iter=1000, tol=1e-3, random_state=42) \n",
    "\n",
    "sgd_model.fit(train_features, train_labels)\n",
    "\n",
    "val_predictions = sgd_model.predict(val_features)\n",
    "test_predictions = sgd_model.predict(test_features)\n",
    "\n",
    "val_accuracy = accuracy_score(val_labels, val_predictions)\n",
    "print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
    "print(\"Validation Classification Report:\")\n",
    "print(classification_report(val_labels, val_predictions))\n",
    "\n",
    "test_accuracy = accuracy_score(test_labels, test_predictions)\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "print(\"Test Classification Report:\")\n",
    "print(classification_report(test_labels, test_predictions))\n",
    "\n",
    "print(\"Validation Confusion Matrix:\")\n",
    "print(confusion_matrix(val_labels, val_predictions))\n",
    "\n",
    "print(\"Test Confusion Matrix:\")\n",
    "print(confusion_matrix(test_labels, test_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "mlp_model = MLPClassifier(hidden_layer_sizes=(100,), max_iter=300, activation='relu', solver='adam', random_state=42)\n",
    "\n",
    "mlp_model.fit(train_features, train_labels)\n",
    "\n",
    "val_predictions = mlp_model.predict(val_features)\n",
    "test_predictions = mlp_model.predict(test_features)\n",
    "\n",
    "val_accuracy = accuracy_score(val_labels, val_predictions)\n",
    "print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
    "print(\"Validation Classification Report:\")\n",
    "print(classification_report(val_labels, val_predictions))\n",
    "\n",
    "test_accuracy = accuracy_score(test_labels, test_predictions)\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "print(\"Test Classification Report:\")\n",
    "print(classification_report(test_labels, test_predictions))\n",
    "\n",
    "print(\"Validation Confusion Matrix:\")\n",
    "print(confusion_matrix(val_labels, val_predictions))\n",
    "\n",
    "print(\"Test Confusion Matrix:\")\n",
    "print(confusion_matrix(test_labels, test_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CatBoost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "\n",
    "catboost_model = CatBoostClassifier(iterations=500, learning_rate=0.1, depth=6, random_seed=42, verbose=0)\n",
    "\n",
    "catboost_model.fit(train_features, train_labels)\n",
    "\n",
    "val_predictions = catboost_model.predict(val_features)\n",
    "test_predictions = catboost_model.predict(test_features)\n",
    "\n",
    "val_accuracy = accuracy_score(val_labels, val_predictions)\n",
    "print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
    "print(\"Validation Classification Report:\")\n",
    "print(classification_report(val_labels, val_predictions))\n",
    "\n",
    "test_accuracy = accuracy_score(test_labels, test_predictions)\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "print(\"Test Classification Report:\")\n",
    "print(classification_report(test_labels, test_predictions))\n",
    "\n",
    "print(\"Validation Confusion Matrix:\")\n",
    "print(confusion_matrix(val_labels, val_predictions))\n",
    "\n",
    "print(\"Test Confusion Matrix:\")\n",
    "print(confusion_matrix(test_labels, test_predictions))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
