{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import GPT2Tokenizer, GPT2Model, BertTokenizer, BertModel, CLIPProcessor, CLIPModel, SwinModel, SwinForImageClassification\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, classification_report\n",
    "import gc\n",
    "import pandas as pd\n",
    "import random\n",
    "import warnings\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", message=\"A parameter name that contains `beta` will be renamed internally to `bias`\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"A parameter name that contains `gamma` will be renamed internally to `weight`\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_path = '/home/umera_p/Dataset/training/memes'\n",
    "\n",
    "def display_images_grid(image_paths, title):\n",
    "    fig, axes = plt.subplots(nrows=10, ncols=5, figsize=(15, 30))\n",
    "    fig.suptitle(title, fontsize=20)\n",
    "    for ax, image_path in zip(axes.flatten(), image_paths):\n",
    "        try:\n",
    "            image = Image.open(image_path)\n",
    "            ax.imshow(image)\n",
    "            ax.axis('off')\n",
    "        except FileNotFoundError:\n",
    "            ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(top=0.95)\n",
    "    plt.show()\n",
    "\n",
    "spanish_ids = [str(i) for i in range(110001, 112035)]\n",
    "english_ids = [str(i) for i in range(210001, 212011)]\n",
    "\n",
    "spanish_images = []\n",
    "for image_id in spanish_ids[:50]:\n",
    "    for ext in ['jpeg', 'jpg', 'png']:\n",
    "        image_path = os.path.join(images_path, f\"{image_id}.{ext}\")\n",
    "        if os.path.exists(image_path):\n",
    "            spanish_images.append(image_path)\n",
    "            break\n",
    "\n",
    "english_images = []\n",
    "for image_id in english_ids[:50]:\n",
    "    for ext in ['jpeg', 'jpg', 'png']:\n",
    "        image_path = os.path.join(images_path, f\"{image_id}.{ext}\")\n",
    "        if os.path.exists(image_path):\n",
    "            english_images.append(image_path)\n",
    "            break\n",
    "\n",
    "display_images_grid(spanish_images, 'First 50 Spanish Images')\n",
    "\n",
    "display_images_grid(english_images, 'First 50 English Images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations_path = '/home/umera_p/Dataset/training/training.json'\n",
    "with open(annotations_path, 'r') as file:\n",
    "    annotations = json.load(file)\n",
    "\n",
    "df = pd.DataFrame.from_dict(annotations, orient='index')\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sexist_count = sum(1 for value in annotations.values() if value['labels_task4'].count('YES')>value['labels_task4'].count('NO'))\n",
    "non_sexist_count = len(annotations) - sexist_count\n",
    "sexist_count_es = sum(1 for value in annotations.values() if value['lang'] == 'es' and value['labels_task4'].count('YES')>value['labels_task4'].count('NO'))\n",
    "non_sexist_count_es = sum(1 for value in annotations.values() if value['lang'] == 'es' and value['labels_task4'].count('YES')<=value['labels_task4'].count('NO'))\n",
    "sexist_count_en = sum(1 for value in annotations.values() if value['lang'] == 'en' and value['labels_task4'].count('YES')>value['labels_task4'].count('NO'))\n",
    "non_sexist_count_en = sum(1 for value in annotations.values() if value['lang'] == 'en' and value['labels_task4'].count('YES')<=value['labels_task4'].count('NO'))\n",
    "\n",
    "print(f\"Total number of images: {len(annotations)}\")\n",
    "print(f\"Number of sexist images: {sexist_count}\")\n",
    "print(f\"Number of sexist images (Spanish): {sexist_count_es}\")\n",
    "print(f\"Number of sexist images (English): {sexist_count_en}\")\n",
    "print(f\"Number of non-sexist images: {non_sexist_count}\")\n",
    "print(f\"Number of non-sexist images (Spanish): {non_sexist_count_es}\")\n",
    "print(f\"Number of non-sexist images (English): {non_sexist_count_en}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MemeDataset(Dataset):\n",
    "    def __init__(self, dataset, base_dir, transform=None, is_test=False):\n",
    "        self.data = dataset\n",
    "        self.keys = list(dataset.keys())\n",
    "        self.base_dir = base_dir\n",
    "        self.transform = transform\n",
    "        self.is_test = is_test\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.keys)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        key = self.keys[idx]\n",
    "        item = self.data[key]\n",
    "        image_path = os.path.join(self.base_dir, item['path_memes'])\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        text = item['text']\n",
    "        \n",
    "        if not self.is_test:\n",
    "            labels = 1 if item['labels_task4'].count('YES')>item['labels_task4'].count('NO') else 0\n",
    "            return image, text, labels\n",
    "        else:\n",
    "            return image, text, key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    images, texts, labels_or_keys = zip(*batch)\n",
    "    images = torch.stack(images, dim=0)\n",
    "\n",
    "    gpt2_tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "    gpt2_tokenizer.pad_token = gpt2_tokenizer.eos_token\n",
    "    gpt2_inputs = gpt2_tokenizer(list(texts), return_tensors='pt', padding=True, truncation=True)\n",
    "\n",
    "    bert_tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "    bert_inputs = bert_tokenizer(list(texts), return_tensors='pt', padding=True, truncation=True)\n",
    "\n",
    "    clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "    clip_text_inputs = clip_processor(text=list(texts), return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    clip_image_inputs = images\n",
    "\n",
    "    if isinstance(labels_or_keys[0], int):\n",
    "        labels = torch.tensor(labels_or_keys).long()\n",
    "        return images, bert_inputs, gpt2_inputs, clip_text_inputs.input_ids, clip_image_inputs, labels\n",
    "    else:\n",
    "        keys = labels_or_keys\n",
    "        return images, bert_inputs, gpt2_inputs, clip_text_inputs.input_ids, clip_image_inputs, keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stratified_split(dataset, split_ratios=(0.8, 0.1, 0.1)):\n",
    "    assert sum(split_ratios) == 1.0, \"Split ratios must sum to 1.0\"\n",
    "\n",
    "    non_sexist_es = [key for key, value in dataset.items() if value['lang'] == 'es' and value['labels_task4'].count('YES')<=value['labels_task4'].count('NO')]\n",
    "    non_sexist_en = [key for key, value in dataset.items() if value['lang'] == 'en' and value['labels_task4'].count('YES')<=value['labels_task4'].count('NO')]\n",
    "    sexist_es = [key for key, value in dataset.items() if value['lang'] == 'es' and value['labels_task4'].count('YES')>value['labels_task4'].count('NO')]\n",
    "    sexist_en = [key for key, value in dataset.items() if value['lang'] == 'en' and value['labels_task4'].count('YES')>value['labels_task4'].count('NO')]\n",
    "\n",
    "    random.shuffle(non_sexist_es)\n",
    "    random.shuffle(non_sexist_en)\n",
    "    random.shuffle(sexist_es)\n",
    "    random.shuffle(sexist_en)\n",
    "\n",
    "    def split_list(data_list, split_ratios):\n",
    "        train_size = round(int(split_ratios[0] * len(data_list)))\n",
    "        val_size = round(int(split_ratios[1] * len(data_list)))\n",
    "        train_split = data_list[:train_size]\n",
    "        val_split = data_list[train_size:train_size + val_size]\n",
    "        test_split = data_list[train_size + val_size:]\n",
    "        return train_split, val_split, test_split\n",
    "\n",
    "    train_non_sexist_es, val_non_sexist_es, test_non_sexist_es = split_list(non_sexist_es, split_ratios)\n",
    "    train_non_sexist_en, val_non_sexist_en, test_non_sexist_en = split_list(non_sexist_en, split_ratios)\n",
    "    train_sexist_es, val_sexist_es, test_sexist_es = split_list(sexist_es, split_ratios)\n",
    "    train_sexist_en, val_sexist_en, test_sexist_en = split_list(sexist_en, split_ratios)\n",
    "\n",
    "    train_keys = train_non_sexist_es + train_non_sexist_en + train_sexist_es + train_sexist_en\n",
    "    val_keys = val_non_sexist_es + val_non_sexist_en + val_sexist_es + val_sexist_en\n",
    "    test_keys = test_non_sexist_es + test_non_sexist_en + test_sexist_es + test_sexist_en\n",
    "\n",
    "    random.shuffle(train_keys)\n",
    "    random.shuffle(val_keys)\n",
    "    random.shuffle(test_keys)\n",
    "\n",
    "    return train_keys, val_keys, test_keys\n",
    "\n",
    "train_keys, val_keys, test_keys = stratified_split(annotations)\n",
    "\n",
    "print(f\"Training set size: {len(train_keys)}\")\n",
    "print(f\"Validation set size: {len(val_keys)}\")\n",
    "print(f\"Known test set size: {len(test_keys)}\")\n",
    "\n",
    "print(\"\\nTraining IDs:\", train_keys)\n",
    "print(\"\\nValidation IDs:\", val_keys)\n",
    "print(\"\\nKnown Test IDs:\", test_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_split_dataset(keys, dataset, base_dir, transform=None):\n",
    "    split_data = {key: dataset[key] for key in keys}\n",
    "    return MemeDataset(split_data, base_dir, transform=transform)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "base_dir = '/home/umera_p/Dataset/training'\n",
    "\n",
    "train_dataset = create_split_dataset(train_keys, annotations, base_dir, transform=transform)\n",
    "val_dataset = create_split_dataset(val_keys, annotations, base_dir, transform=transform)\n",
    "known_test_dataset = create_split_dataset(test_keys, annotations, base_dir, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False, collate_fn=collate_fn)\n",
    "known_test_loader = DataLoader(known_test_dataset, batch_size=8, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultimodalModel(nn.Module):\n",
    "    def __init__(self, bert_model, gpt2_model, clip_model, swin_model, hidden_dim=512, num_classes=2):\n",
    "        super(MultimodalModel, self).__init__()\n",
    "        self.bert_model = bert_model\n",
    "        self.gpt2_model = gpt2_model\n",
    "        self.clip_model = clip_model\n",
    "        self.swin_model = swin_model\n",
    "\n",
    "        self.bert_fc = nn.Linear(bert_model.config.hidden_size, hidden_dim)\n",
    "        self.gpt2_fc = nn.Linear(gpt2_model.config.hidden_size, hidden_dim) \n",
    "        self.clip_fc = nn.Linear(clip_model.config.text_config.hidden_size, hidden_dim)\n",
    "        self.swin_fc = nn.Linear(swin_model.num_labels, hidden_dim)\n",
    "\n",
    "        self.text_fusion_fc = nn.Linear(hidden_dim * 3, hidden_dim)\n",
    "        self.visual_fusion_fc = nn.Linear(hidden_dim, hidden_dim)\n",
    "\n",
    "        self.linear1 = nn.Linear(hidden_dim * 2, hidden_dim)\n",
    "        self.linear2=nn.Linear(hidden_dim,hidden_dim//2)\n",
    "        self.linear3=nn.Linear(hidden_dim//2,hidden_dim//4)\n",
    "        self.classifier=nn.Linear(hidden_dim//4,num_classes)\n",
    "\n",
    "    def forward(self, images, bert_input, gpt2_input, clip_text_input, clip_image_input):\n",
    "        bert_features = self.bert_fc(self.bert_model(**bert_input).last_hidden_state[:, 0, :])\n",
    "        gpt2_features = self.gpt2_fc(self.gpt2_model(**gpt2_input).last_hidden_state[:, -1, :])\n",
    "        clip_text_features = self.clip_fc(self.clip_model.get_text_features(clip_text_input))\n",
    "        clip_image_features = self.swin_fc(self.swin_model(images).logits)\n",
    "\n",
    "        text_features = torch.cat((bert_features, gpt2_features, clip_text_features), dim=1)\n",
    "        visual_features = clip_image_features\n",
    "\n",
    "        text_fusion = self.text_fusion_fc(text_features)\n",
    "        visual_fusion = self.visual_fusion_fc(visual_features)\n",
    "\n",
    "        fused_features = torch.cat((text_fusion, visual_fusion), dim=1)\n",
    "        output = self.classifier(F.relu(self.linear3(F.relu(self.linear2(F.relu(self.linear1(fused_features)))))))\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_model(model, val_loader, criterion):\n",
    "    model.eval()\n",
    "    val_running_loss = 0.0\n",
    "    val_all_labels = []\n",
    "    val_all_preds = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, bert_input, gpt2_input, clip_text_input, clip_image_input, labels in val_loader:\n",
    "            images = images.to(device)\n",
    "            bert_input = {key: val.to(device) for key, val in bert_input.items()}\n",
    "            gpt2_input = {key: val.to(device) for key, val in gpt2_input.items()}\n",
    "            clip_text_input = clip_text_input.to(device)\n",
    "            clip_image_input = clip_image_input.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(images, bert_input, gpt2_input, clip_text_input, clip_image_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            val_running_loss += loss.item()\n",
    "\n",
    "            _, preds = torch.max(outputs, dim=1)\n",
    "            val_all_preds.extend(preds.cpu().numpy())\n",
    "            val_all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    avg_val_loss = val_running_loss / len(val_loader)\n",
    "    val_accuracy = accuracy_score(val_all_labels, val_all_preds)\n",
    "    val_f1 = f1_score(val_all_labels, val_all_preds)\n",
    "\n",
    "    val_conf_matrix = confusion_matrix(val_all_labels, val_all_preds)\n",
    "\n",
    "    return avg_val_loss, val_accuracy, val_f1, val_all_labels, val_all_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, known_test_loader, num_epochs=20):\n",
    "    model = model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-4,weight_decay=1e-5)\n",
    "    scheduler=optim.lr_scheduler.ReduceLROnPlateau(optimizer)\n",
    "\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        train_all_preds = []\n",
    "        train_all_labels = []\n",
    "\n",
    "        for images, bert_input, gpt2_input, clip_text_input, clip_image_input, labels in train_loader:\n",
    "            images = images.to(device)\n",
    "            bert_input = {key: val.to(device) for key, val in bert_input.items()}\n",
    "            gpt2_input = {key: val.to(device) for key, val in gpt2_input.items()}\n",
    "            clip_text_input = clip_text_input.to(device)\n",
    "            clip_image_input = clip_image_input.to(device)\n",
    "            labels = labels.to(device).long()\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images, bert_input, gpt2_input, clip_text_input, clip_image_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            _, preds = torch.max(outputs, dim=1)\n",
    "            train_all_preds.extend(preds.cpu().numpy())\n",
    "            train_all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        avg_loss = running_loss / len(train_loader)\n",
    "        accuracy = accuracy_score(train_all_labels, train_all_preds)\n",
    "        f1 = f1_score(train_all_labels, train_all_preds)\n",
    "\n",
    "        conf_matrix = confusion_matrix(train_all_labels, train_all_preds)\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "        print(f\"Training Loss: {avg_loss:.4f} | Training Accuracy: {accuracy:.4f}\")\n",
    "        print(\"Training Confusion Matrix:\")\n",
    "        print(conf_matrix)\n",
    "        print(f\"Training F1 Score: {f1}\")\n",
    "\n",
    "        val_loss, val_accuracy, val_f1, val_all_labels, val_all_preds = validate_model(model, val_loader, criterion)\n",
    "        scheduler.step(val_loss)\n",
    "        print(f\"Validation Loss: {val_loss:.4f} | Validation Accuracy: {val_accuracy:.4f}\")\n",
    "        print(\"Validation Confusion Matrix:\")\n",
    "        print(confusion_matrix(val_all_labels, val_all_preds))\n",
    "        print(f\"Validation F1 Score: {val_f1:.4f}\")\n",
    "\n",
    "        torch.save(model.state_dict(), f'model_epoch_{epoch+1}.pth')\n",
    "        print(f\"Checkpoint saved for epoch {epoch+1}\")\n",
    "          \n",
    "        gc.collect()\n",
    "\n",
    "    print(\"\\nFinal Training Classification Report:\")\n",
    "    print(classification_report(train_all_labels, train_all_preds, target_names=['Non-Sexist', 'Sexist']))\n",
    "\n",
    "    print(\"\\nFinal Validation Classification Report:\")\n",
    "    print(classification_report(val_all_labels, val_all_preds, target_names=['Non-Sexist', 'Sexist']))\n",
    "\n",
    "    test_loss, test_accuracy, test_f1, test_labels, test_preds = validate_model(model, known_test_loader, criterion)\n",
    "    print(f\"Known Test Loss: {test_loss:.4f} | Known Test Accuracy: {test_accuracy:.4f}\")\n",
    "    print(\"Known Test Confusion Matrix:\")\n",
    "    print(confusion_matrix(test_labels, test_preds))\n",
    "    print(f\"Known Test F1 Score: {test_f1:.4f}\")\n",
    "\n",
    "    print(\"\\nFinal Validation on Known Test Set\")\n",
    "    test_loss, test_accuracy, test_f1, test_labels, test_preds = validate_model(model, known_test_loader, criterion)\n",
    "    print(f\"Final Known Test Loss: {test_loss:.4f} | Final Known Test Accuracy: {test_accuracy:.4f}\")\n",
    "    print(\"Final Known Test Confusion Matrix:\")\n",
    "    print(confusion_matrix(test_labels, test_preds))\n",
    "    print(f\"Final Known Test F1 Score: {test_f1:.4f}\")\n",
    "    print(\"\\nFinal Known Test Classification Report:\")\n",
    "    print(classification_report(test_labels, test_preds, target_names=['Non-Sexist', 'Sexist']))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model = BertModel.from_pretrained('bert-base-multilingual-cased')\n",
    "gpt2_model = GPT2Model.from_pretrained('gpt2')\n",
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "swin_model = SwinForImageClassification.from_pretrained('microsoft/swin-tiny-patch4-window7-224')\n",
    "for param in bert_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "for param in gpt2_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "for param in clip_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "for param in swin_model.parameters():\n",
    "    param.requires_grad = False\n",
    "swin_model.to('cuda:1')\n",
    "gpt2_model.to('cuda:1')\n",
    "bert_model.to('cuda:1')\n",
    "clip_model.to('cuda:1')\n",
    "\n",
    "model = nn.DataParallel(MultimodalModel(bert_model, gpt2_model, clip_model, swin_model), device_ids=[1])\n",
    "\n",
    "model.to('cuda:1')\n",
    "\n",
    "train_model(model, train_loader, val_loader, known_test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_annotations_path = '/home/umera_p/Dataset/test/test_clean.json'\n",
    "with open(test_annotations_path, 'r') as file:\n",
    "    test_annotations = json.load(file)\n",
    "\n",
    "test_base_dir = '/home/umera_p/Dataset/test'\n",
    "test_dataset = MemeDataset(\n",
    "    dataset=test_annotations, \n",
    "    base_dir=test_base_dir,\n",
    "    transform=transform,\n",
    "    is_test=True\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=2, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "def predict_model(model, test_loader):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    image_ids = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, bert_input, gpt2_input, clip_text_input, clip_image_input, keys in tqdm(test_loader):\n",
    "            images = images.to(device)\n",
    "            bert_input = {key: val.to(device) for key, val in bert_input.items()}\n",
    "            gpt2_input = {key: val.to(device) for key, val in gpt2_input.items()}\n",
    "            clip_text_input = clip_text_input.to(device)\n",
    "            clip_image_input = clip_image_input.to(device)\n",
    "\n",
    "            outputs = model(images, bert_input, gpt2_input, clip_text_input, clip_image_input)\n",
    "            _, preds = torch.max(outputs, dim=1)\n",
    "\n",
    "            predictions.extend(preds.cpu().numpy())\n",
    "            image_ids.extend(keys)\n",
    "\n",
    "    return image_ids, predictions\n",
    "\n",
    "image_ids, predictions = predict_model(model, test_loader)\n",
    "\n",
    "for img_id, prediction in zip(image_ids, predictions):\n",
    "    print(f\"Image ID: {img_id} - Prediction: {'Sexist' if prediction == 1 else 'Non-Sexist'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def save_predictions_json(image_ids, predictions, overall_path, sexist_path, non_sexist_path):\n",
    "    results = []\n",
    "    for img_id, pred in zip(image_ids, predictions):\n",
    "        results.append({'image_id': img_id, 'prediction': 'Sexist' if pred == 1 else 'Non-Sexist'})\n",
    "    \n",
    "    with open(overall_path, 'w') as f:\n",
    "        json.dump(results, f, indent=4)\n",
    "    \n",
    "    sexist_results = [r for r in results if r['prediction'] == 'Sexist']\n",
    "    non_sexist_results = [r for r in results if r['prediction'] == 'Non-Sexist']\n",
    "    \n",
    "    with open(sexist_path, 'w') as f:\n",
    "        json.dump(sexist_results, f, indent=4)\n",
    "    \n",
    "    with open(non_sexist_path, 'w') as f:\n",
    "        json.dump(non_sexist_results, f, indent=4)\n",
    "\n",
    "save_predictions_json(image_ids, predictions, 'Task_4_overall_predictions.json', 'Task_4_sexist_predictions.json', 'Task_4_non_sexist_predictions.json')\n",
    "\n",
    "def load_predictions_json(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        return json.load(f)\n",
    "overall_results = load_predictions_json('Task_4_overall_predictions.json')  \n",
    "print(\"Overall Predictions :\")\n",
    "print(overall_results)\n",
    "sexist_results = load_predictions_json('Task_4_sexist_predictions.json')\n",
    "print(\"Sexist Predictions :\")\n",
    "print(sexist_results)\n",
    "non_sexist_results = load_predictions_json('Task_4_non_sexist_predictions.json')\n",
    "print(\"Non-Sexist Predictions :\")\n",
    "print(non_sexist_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
